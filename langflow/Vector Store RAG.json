{"id":"4e52270a-4898-482a-b626-0a9b4634b255","data":{"nodes":[{"data":{"description":"Get chat inputs from the Playground.","display_name":"Chat Input","id":"ChatInput-pcMgt","node":{"base_classes":["Message"],"beta":false,"conditional_paths":[],"custom_fields":{},"description":"Get chat inputs from the Playground.","display_name":"Chat Input","documentation":"","edited":false,"field_order":["input_value","should_store_message","sender","sender_name","session_id","files"],"frozen":false,"icon":"ChatInput","output_types":[],"outputs":[{"cache":true,"display_name":"Message","method":"message_response","name":"message","selected":"Message","types":["Message"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"User\",\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=\"User\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n"},"files":{"advanced":true,"display_name":"Files","dynamic":false,"fileTypes":["txt","md","mdx","csv","json","yaml","yml","xml","html","htm","pdf","docx","py","sh","sql","js","ts","tsx","jpg","jpeg","png","bmp","image"],"file_path":"","info":"Files to be sent with the message.","list":true,"name":"files","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"file","value":""},"input_value":{"advanced":false,"display_name":"Text","dynamic":false,"info":"Message to be passed as input.","input_types":["Message"],"list":false,"load_from_db":false,"multiline":true,"name":"input_value","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"summerize the items that is accepted by rumpke recycling"},"sender":{"advanced":true,"display_name":"Sender Type","dynamic":false,"info":"Type of sender.","name":"sender","options":["Machine","User"],"placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"str","value":"User"},"sender_name":{"advanced":true,"display_name":"Sender Name","dynamic":false,"info":"Name of the sender.","input_types":["Message"],"list":false,"load_from_db":false,"name":"sender_name","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"User"},"session_id":{"advanced":true,"display_name":"Session ID","dynamic":false,"info":"Session ID for the message.","input_types":["Message"],"list":false,"load_from_db":false,"name":"session_id","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"should_store_message":{"advanced":true,"display_name":"Store Messages","dynamic":false,"info":"Store the message in the history.","list":false,"name":"should_store_message","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":true}}},"type":"ChatInput"},"dragging":false,"height":316,"id":"ChatInput-pcMgt","position":{"x":642.3545710150049,"y":220.22556606238678},"positionAbsolute":{"x":642.3545710150049,"y":220.22556606238678},"selected":false,"type":"genericNode","width":384},{"data":{"description":"Convert Data into plain text following a specified template.","display_name":"Parse Data","id":"ParseData-QUatH","node":{"base_classes":["Message"],"beta":false,"conditional_paths":[],"custom_fields":{},"description":"Convert Data into plain text following a specified template.","display_name":"Parse Data","documentation":"","edited":false,"field_order":["data","template","sep"],"frozen":false,"icon":"braces","output_types":[],"outputs":[{"cache":true,"display_name":"Text","method":"parse_data","name":"text","selected":"Message","types":["Message"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import DataInput, MultilineInput, Output, StrInput\nfrom langflow.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n    name = \"ParseData\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n"},"data":{"advanced":false,"display_name":"Data","dynamic":false,"info":"The data to convert to text.","input_types":["Data"],"list":false,"name":"data","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"other","value":""},"sep":{"advanced":true,"display_name":"Separator","dynamic":false,"info":"","list":false,"load_from_db":false,"name":"sep","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"str","value":"\n"},"template":{"advanced":false,"display_name":"Template","dynamic":false,"info":"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.","input_types":["Message"],"list":false,"load_from_db":false,"multiline":true,"name":"template","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"{text}"}}},"type":"ParseData"},"dragging":false,"height":400,"id":"ParseData-QUatH","position":{"x":1847.6826284522963,"y":297.6086089304885},"positionAbsolute":{"x":1847.6826284522963,"y":297.6086089304885},"selected":false,"type":"genericNode","width":384},{"data":{"description":"Create a prompt template with dynamic variables.","display_name":"Prompt","id":"Prompt-nwpd4","node":{"base_classes":["Message"],"beta":false,"conditional_paths":[],"custom_fields":{"template":["context","question"]},"description":"Create a prompt template with dynamic variables.","display_name":"Prompt","documentation":"","edited":false,"field_order":["template"],"frozen":false,"icon":"prompts","output_types":[],"outputs":[{"cache":true,"display_name":"Prompt Message","method":"build_prompt","name":"prompt","selected":"Message","types":["Message"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n"},"context":{"advanced":false,"display_name":"context","dynamic":false,"field_type":"str","fileTypes":[],"file_path":"","info":"","input_types":["Message","Text"],"list":false,"load_from_db":false,"multiline":true,"name":"context","password":false,"placeholder":"","required":false,"show":true,"title_case":false,"type":"str","value":""},"question":{"advanced":false,"display_name":"question","dynamic":false,"field_type":"str","fileTypes":[],"file_path":"","info":"","input_types":["Message","Text"],"list":false,"load_from_db":false,"multiline":true,"name":"question","password":false,"placeholder":"","required":false,"show":true,"title_case":false,"type":"str","value":""},"template":{"advanced":false,"display_name":"Template","dynamic":false,"info":"","list":false,"load_from_db":false,"name":"template","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"type":"prompt","value":"{context}\n\n---\n\nGiven the context above, answer the question as best as possible.\n\nQuestion: {question}\n\nAnswer: "}}},"type":"Prompt"},"dragging":false,"height":523,"id":"Prompt-nwpd4","position":{"x":2486.0988668404975,"y":496.5120474157301},"positionAbsolute":{"x":2486.0988668404975,"y":496.5120474157301},"selected":false,"type":"genericNode","width":384},{"data":{"description":"Display a chat message in the Playground.","display_name":"Chat Output","id":"ChatOutput-c1Jrr","node":{"base_classes":["Message"],"beta":false,"conditional_paths":[],"custom_fields":{},"description":"Display a chat message in the Playground.","display_name":"Chat Output","documentation":"","edited":false,"field_order":["input_value","should_store_message","sender","sender_name","session_id","data_template"],"frozen":false,"icon":"ChatOutput","output_types":[],"outputs":[{"cache":true,"display_name":"Message","method":"message_response","name":"message","selected":"Message","types":["Message"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"Machine\",\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\", display_name=\"Sender Name\", info=\"Name of the sender.\", value=\"AI\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n"},"data_template":{"advanced":true,"display_name":"Data Template","dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","input_types":["Message"],"list":false,"load_from_db":false,"name":"data_template","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"{text}"},"input_value":{"advanced":false,"display_name":"Text","dynamic":false,"info":"Message to be passed as output.","input_types":["Message"],"list":false,"load_from_db":false,"name":"input_value","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"sender":{"advanced":true,"display_name":"Sender Type","dynamic":false,"info":"Type of sender.","name":"sender","options":["Machine","User"],"placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"str","value":"Machine"},"sender_name":{"advanced":true,"display_name":"Sender Name","dynamic":false,"info":"Name of the sender.","input_types":["Message"],"list":false,"load_from_db":false,"name":"sender_name","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"AI"},"session_id":{"advanced":true,"display_name":"Session ID","dynamic":false,"info":"Session ID for the message.","input_types":["Message"],"list":false,"load_from_db":false,"name":"session_id","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"should_store_message":{"advanced":true,"display_name":"Store Messages","dynamic":false,"info":"Store the message in the history.","list":false,"name":"should_store_message","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":true}}},"type":"ChatOutput"},"dragging":false,"height":316,"id":"ChatOutput-c1Jrr","position":{"x":3769.242086248817,"y":585.3403837062634},"positionAbsolute":{"x":3769.242086248817,"y":585.3403837062634},"selected":false,"type":"genericNode","width":384},{"data":{"description":"Split text into chunks based on specified criteria.","display_name":"Split Text","id":"SplitText-uDC7v","node":{"base_classes":["Data"],"beta":false,"conditional_paths":[],"custom_fields":{},"description":"Split text into chunks based on specified criteria.","display_name":"Split Text","documentation":"","edited":false,"field_order":["data_inputs","chunk_overlap","chunk_size","separator"],"frozen":false,"icon":"scissors-line-dashed","output_types":[],"outputs":[{"cache":true,"display_name":"Chunks","method":"split_text","name":"chunks","selected":"Data","types":["Data"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","chunk_overlap":{"advanced":false,"display_name":"Chunk Overlap","dynamic":false,"info":"Number of characters to overlap between chunks.","list":false,"name":"chunk_overlap","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"int","value":"200"},"chunk_size":{"advanced":false,"display_name":"Chunk Size","dynamic":false,"info":"The maximum number of characters in each chunk.","list":false,"name":"chunk_size","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"int","value":"1000"},"code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from typing import List\n\nfrom langchain_text_splitters import CharacterTextSplitter\n\nfrom langflow.custom import Component\nfrom langflow.io import HandleInput, IntInput, MessageTextInput, Output\nfrom langflow.schema import Data\nfrom langflow.utils.util import unescape_string\n\n\nclass SplitTextComponent(Component):\n    display_name: str = \"Split Text\"\n    description: str = \"Split text into chunks based on specified criteria.\"\n    icon = \"scissors-line-dashed\"\n    name = \"SplitText\"\n\n    inputs = [\n        HandleInput(\n            name=\"data_inputs\",\n            display_name=\"Data Inputs\",\n            info=\"The data to split.\",\n            input_types=[\"Data\"],\n            is_list=True,\n        ),\n        IntInput(\n            name=\"chunk_overlap\",\n            display_name=\"Chunk Overlap\",\n            info=\"Number of characters to overlap between chunks.\",\n            value=200,\n        ),\n        IntInput(\n            name=\"chunk_size\",\n            display_name=\"Chunk Size\",\n            info=\"The maximum number of characters in each chunk.\",\n            value=1000,\n        ),\n        MessageTextInput(\n            name=\"separator\",\n            display_name=\"Separator\",\n            info=\"The character to split on. Defaults to newline.\",\n            value=\"\\n\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Chunks\", name=\"chunks\", method=\"split_text\"),\n    ]\n\n    def _docs_to_data(self, docs):\n        data = []\n        for doc in docs:\n            data.append(Data(text=doc.page_content, data=doc.metadata))\n        return data\n\n    def split_text(self) -> List[Data]:\n        separator = unescape_string(self.separator)\n\n        documents = []\n        for _input in self.data_inputs:\n            if isinstance(_input, Data):\n                documents.append(_input.to_lc_document())\n\n        splitter = CharacterTextSplitter(\n            chunk_overlap=self.chunk_overlap,\n            chunk_size=self.chunk_size,\n            separator=separator,\n        )\n        docs = splitter.split_documents(documents)\n        data = self._docs_to_data(docs)\n        self.status = data\n        return data\n"},"data_inputs":{"advanced":false,"display_name":"Data Inputs","dynamic":false,"info":"The data to split.","input_types":["Data"],"list":true,"name":"data_inputs","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"other","value":""},"separator":{"advanced":false,"display_name":"Separator","dynamic":false,"info":"The character to split on. Defaults to newline.","input_types":["Message"],"list":false,"load_from_db":false,"name":"separator","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"\n"}}},"type":"SplitText"},"dragging":false,"height":543,"id":"SplitText-uDC7v","position":{"x":2048.0893802005144,"y":1223.407676597908},"positionAbsolute":{"x":2048.0893802005144,"y":1223.407676597908},"selected":false,"type":"genericNode","width":384},{"id":"OllamaModel-OKERI","type":"genericNode","position":{"x":3121.179834735751,"y":305.8861555881754},"data":{"type":"OllamaModel","node":{"template":{"_type":"Component","base_url":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"http://ollama:11434","name":"base_url","display_name":"Base URL","advanced":false,"dynamic":false,"info":"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.","title_case":false,"type":"str","_input_type":"StrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import Any\n\nimport httpx\nfrom langchain_community.chat_models import ChatOllama\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, StrInput\n\n\nclass ChatOllamaComponent(LCModelComponent):\n    display_name = \"Ollama\"\n    description = \"Generate text using Ollama Local LLMs.\"\n    icon = \"Ollama\"\n    name = \"OllamaModel\"\n\n    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"mirostat\":\n            if field_value == \"Disabled\":\n                build_config[\"mirostat_eta\"][\"advanced\"] = True\n                build_config[\"mirostat_tau\"][\"advanced\"] = True\n                build_config[\"mirostat_eta\"][\"value\"] = None\n                build_config[\"mirostat_tau\"][\"value\"] = None\n\n            else:\n                build_config[\"mirostat_eta\"][\"advanced\"] = False\n                build_config[\"mirostat_tau\"][\"advanced\"] = False\n\n                if field_value == \"Mirostat 2.0\":\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.2\n                    build_config[\"mirostat_tau\"][\"value\"] = 10\n                else:\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.1\n                    build_config[\"mirostat_tau\"][\"value\"] = 5\n\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model_name\"][\"options\"] = self.get_model(base_url_value + \"/api/tags\")\n\n        if field_name == \"keep_alive_flag\":\n            if field_value == \"Keep\":\n                build_config[\"keep_alive\"][\"value\"] = \"-1\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            elif field_value == \"Immediately\":\n                build_config[\"keep_alive\"][\"value\"] = \"0\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            else:\n                build_config[\"keep_alive\"][\"advanced\"] = False\n\n        return build_config\n\n    def get_model(self, url: str) -> list[str]:\n        try:\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                model_names = [model[\"name\"] for model in data.get(\"models\", [])]\n                return model_names\n        except Exception as e:\n            raise ValueError(\"Could not retrieve models. Please, make sure Ollama is running.\") from e\n\n    inputs = LCModelComponent._base_inputs + [\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"llama3\",\n            info=\"Refer to https://ollama.ai/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.2,\n            info=\"Controls the creativity of model responses.\",\n        ),\n        StrInput(\n            name=\"format\",\n            display_name=\"Format\",\n            info=\"Specify the format of the output (e.g., json).\",\n            advanced=True,\n        ),\n        DictInput(\n            name=\"metadata\",\n            display_name=\"Metadata\",\n            info=\"Metadata to add to the run trace.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"mirostat\",\n            display_name=\"Mirostat\",\n            options=[\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\n            info=\"Enable/disable Mirostat sampling for controlling perplexity.\",\n            value=\"Disabled\",\n            advanced=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"mirostat_eta\",\n            display_name=\"Mirostat Eta\",\n            info=\"Learning rate for Mirostat algorithm. (Default: 0.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"mirostat_tau\",\n            display_name=\"Mirostat Tau\",\n            info=\"Controls the balance between coherence and diversity of the output. (Default: 5.0)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_ctx\",\n            display_name=\"Context Window Size\",\n            info=\"Size of the context window for generating tokens. (Default: 2048)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_gpu\",\n            display_name=\"Number of GPUs\",\n            info=\"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_thread\",\n            display_name=\"Number of Threads\",\n            info=\"Number of threads to use during computation. (Default: detected for optimal performance)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"repeat_last_n\",\n            display_name=\"Repeat Last N\",\n            info=\"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"tfs_z\",\n            display_name=\"TFS Z\",\n            info=\"Tail free sampling value. (Default: 1)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            info=\"Timeout for the request stream.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Limits token selection to top K. (Default: 40)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"Works together with top-k. (Default: 0.9)\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"verbose\",\n            display_name=\"Verbose\",\n            info=\"Whether to print out response text.\",\n        ),\n        StrInput(\n            name=\"tags\",\n            display_name=\"Tags\",\n            info=\"Comma-separated list of tags to add to the run trace.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"system\",\n            display_name=\"System\",\n            info=\"System to use for generating text.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"Template to use for generating text.\",\n            advanced=True,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # Mapping mirostat settings to their corresponding values\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\n\n        # Default to 0 for 'Disabled'\n        mirostat_value = mirostat_options.get(self.mirostat, 0)  # type: ignore\n\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\n        if mirostat_value == 0:\n            mirostat_eta = None\n            mirostat_tau = None\n        else:\n            mirostat_eta = self.mirostat_eta\n            mirostat_tau = self.mirostat_tau\n\n        # Mapping system settings to their corresponding values\n        llm_params = {\n            \"base_url\": self.base_url,\n            \"model\": self.model_name,\n            \"mirostat\": mirostat_value,\n            \"format\": self.format,\n            \"metadata\": self.metadata,\n            \"tags\": self.tags.split(\",\") if self.tags else None,\n            \"mirostat_eta\": mirostat_eta,\n            \"mirostat_tau\": mirostat_tau,\n            \"num_ctx\": self.num_ctx or None,\n            \"num_gpu\": self.num_gpu or None,\n            \"num_thread\": self.num_thread or None,\n            \"repeat_last_n\": self.repeat_last_n or None,\n            \"repeat_penalty\": self.repeat_penalty or None,\n            \"temperature\": self.temperature or None,\n            \"stop\": self.stop_tokens.split(\",\") if self.stop_tokens else None,\n            \"system\": self.system,\n            \"template\": self.template,\n            \"tfs_z\": self.tfs_z or None,\n            \"timeout\": self.timeout or None,\n            \"top_k\": self.top_k or None,\n            \"top_p\": self.top_p or None,\n            \"verbose\": self.verbose,\n        }\n\n        # Remove parameters with None values\n        llm_params = {k: v for k, v in llm_params.items() if v is not None}\n\n        try:\n            output = ChatOllama(**llm_params)  # type: ignore\n        except Exception as e:\n            raise ValueError(\"Could not initialize Ollama LLM.\") from e\n\n        return output  # type: ignore\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"format":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"format","display_name":"Format","advanced":true,"dynamic":false,"info":"Specify the format of the output (e.g., json).","title_case":false,"type":"str","_input_type":"StrInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"metadata":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":{},"name":"metadata","display_name":"Metadata","advanced":true,"dynamic":false,"info":"Metadata to add to the run trace.","title_case":false,"type":"dict","_input_type":"DictInput"},"mirostat":{"trace_as_metadata":true,"options":["Disabled","Mirostat","Mirostat 2.0"],"combobox":false,"required":false,"placeholder":"","show":true,"value":"Disabled","name":"mirostat","display_name":"Mirostat","advanced":true,"dynamic":false,"info":"Enable/disable Mirostat sampling for controlling perplexity.","real_time_refresh":true,"title_case":false,"type":"str","_input_type":"DropdownInput"},"mirostat_eta":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"mirostat_eta","display_name":"Mirostat Eta","advanced":true,"dynamic":false,"info":"Learning rate for Mirostat algorithm. (Default: 0.1)","title_case":false,"type":"float","_input_type":"FloatInput"},"mirostat_tau":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"mirostat_tau","display_name":"Mirostat Tau","advanced":true,"dynamic":false,"info":"Controls the balance between coherence and diversity of the output. (Default: 5.0)","title_case":false,"type":"float","_input_type":"FloatInput"},"model_name":{"trace_as_metadata":true,"options":["llama3:8b"],"combobox":false,"required":false,"placeholder":"","show":true,"value":"llama3:8b","name":"model_name","display_name":"Model Name","advanced":false,"dynamic":false,"info":"Refer to https://ollama.ai/library for more models.","refresh_button":true,"title_case":false,"type":"str","_input_type":"DropdownInput"},"num_ctx":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"num_ctx","display_name":"Context Window Size","advanced":true,"dynamic":false,"info":"Size of the context window for generating tokens. (Default: 2048)","title_case":false,"type":"int","_input_type":"IntInput"},"num_gpu":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"num_gpu","display_name":"Number of GPUs","advanced":true,"dynamic":false,"info":"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)","title_case":false,"type":"int","_input_type":"IntInput"},"num_thread":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"num_thread","display_name":"Number of Threads","advanced":true,"dynamic":false,"info":"Number of threads to use during computation. (Default: detected for optimal performance)","title_case":false,"type":"int","_input_type":"IntInput"},"repeat_last_n":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"repeat_last_n","display_name":"Repeat Last N","advanced":true,"dynamic":false,"info":"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)","title_case":false,"type":"int","_input_type":"IntInput"},"repeat_penalty":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"repeat_penalty","display_name":"Repeat Penalty","advanced":true,"dynamic":false,"info":"Penalty for repetitions in generated text. (Default: 1.1)","title_case":false,"type":"float","_input_type":"FloatInput"},"stop_tokens":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"stop_tokens","display_name":"Stop Tokens","advanced":true,"dynamic":false,"info":"Comma-separated list of tokens to signal the model to stop generating text.","title_case":false,"type":"str","_input_type":"StrInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"stream","display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"system","display_name":"System","advanced":true,"dynamic":false,"info":"System to use for generating text.","title_case":false,"type":"str","_input_type":"StrInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"system_message","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"tags":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"tags","display_name":"Tags","advanced":true,"dynamic":false,"info":"Comma-separated list of tags to add to the run trace.","title_case":false,"type":"str","_input_type":"StrInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":0.2,"name":"temperature","display_name":"Temperature","advanced":false,"dynamic":false,"info":"Controls the creativity of model responses.","title_case":false,"type":"float","_input_type":"FloatInput"},"template":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"template","display_name":"Template","advanced":true,"dynamic":false,"info":"Template to use for generating text.","title_case":false,"type":"str","_input_type":"StrInput"},"tfs_z":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"tfs_z","display_name":"TFS Z","advanced":true,"dynamic":false,"info":"Tail free sampling value. (Default: 1)","title_case":false,"type":"float","_input_type":"FloatInput"},"timeout":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"timeout","display_name":"Timeout","advanced":true,"dynamic":false,"info":"Timeout for the request stream.","title_case":false,"type":"int","_input_type":"IntInput"},"top_k":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"top_k","display_name":"Top K","advanced":true,"dynamic":false,"info":"Limits token selection to top K. (Default: 40)","title_case":false,"type":"int","_input_type":"IntInput"},"top_p":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"top_p","display_name":"Top P","advanced":true,"dynamic":false,"info":"Works together with top-k. (Default: 0.9)","title_case":false,"type":"float","_input_type":"FloatInput"},"verbose":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"verbose","display_name":"Verbose","advanced":false,"dynamic":false,"info":"Whether to print out response text.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Generate text using Ollama Local LLMs.","icon":"Ollama","base_classes":["LanguageModel","Message"],"display_name":"Ollama","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","system_message","stream","base_url","model_name","temperature","format","metadata","mirostat","mirostat_eta","mirostat_tau","num_ctx","num_gpu","num_thread","repeat_last_n","repeat_penalty","tfs_z","timeout","top_k","top_p","verbose","tags","stop_tokens","system","template"],"beta":false,"edited":false},"id":"OllamaModel-OKERI"},"selected":false,"width":384,"height":709,"positionAbsolute":{"x":3121.179834735751,"y":305.8861555881754},"dragging":false},{"id":"Cassandra-GJ8X6","type":"genericNode","position":{"x":3036.2877189579776,"y":1494.2887273066433},"data":{"type":"Cassandra","node":{"template":{"_type":"Component","embedding":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"embedding","display_name":"Embedding","advanced":false,"input_types":["Embeddings"],"dynamic":false,"info":"","title_case":false,"type":"other","_input_type":"HandleInput"},"ingest_data":{"trace_as_input":true,"trace_as_metadata":true,"list":true,"required":false,"placeholder":"","show":true,"value":"","name":"ingest_data","display_name":"Ingest Data","advanced":false,"input_types":["Data"],"dynamic":false,"info":"","title_case":false,"type":"other","_input_type":"DataInput"},"batch_size":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":16,"name":"batch_size","display_name":"Batch Size","advanced":true,"dynamic":false,"info":"Optional number of data to process in a single batch.","title_case":false,"type":"int","_input_type":"IntInput"},"body_search":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"body_search","display_name":"Search Body","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Document textual search terms to apply to the search query.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"cluster_kwargs":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"value":{},"name":"cluster_kwargs","display_name":"Cluster arguments","advanced":true,"dynamic":false,"info":"Optional dictionary of additional keyword arguments for the Cassandra cluster.","title_case":false,"type":"dict","_input_type":"DictInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import List\n\nfrom langchain_community.vectorstores import Cassandra\nfrom loguru import logger\n\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.inputs import BoolInput, DictInput, FloatInput\nfrom langflow.io import (\n    DataInput,\n    DropdownInput,\n    HandleInput,\n    IntInput,\n    MessageTextInput,\n    MultilineInput,\n    SecretStrInput,\n)\nfrom langflow.schema import Data\n\n\nclass CassandraVectorStoreComponent(LCVectorStoreComponent):\n    display_name = \"Cassandra\"\n    description = \"Cassandra Vector Store with search capabilities\"\n    documentation = \"https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/cassandra\"\n    name = \"Cassandra\"\n    icon = \"Cassandra\"\n\n    _cached_vectorstore: Cassandra | None = None\n\n    inputs = [\n        MessageTextInput(\n            name=\"database_ref\",\n            display_name=\"Contact Points / Astra Database ID\",\n            info=\"Contact points for the database (or AstraDB database ID)\",\n            required=True,\n        ),\n        MessageTextInput(\n            name=\"username\", display_name=\"Username\", info=\"Username for the database (leave empty for AstraDB).\"\n        ),\n        SecretStrInput(\n            name=\"token\",\n            display_name=\"Password / AstraDB Token\",\n            info=\"User password for the database (or AstraDB token).\",\n            required=True,\n        ),\n        MessageTextInput(\n            name=\"keyspace\",\n            display_name=\"Keyspace\",\n            info=\"Table Keyspace (or AstraDB namespace).\",\n            required=True,\n        ),\n        MessageTextInput(\n            name=\"table_name\",\n            display_name=\"Table Name\",\n            info=\"The name of the table (or AstraDB collection) where vectors will be stored.\",\n            required=True,\n        ),\n        IntInput(\n            name=\"ttl_seconds\",\n            display_name=\"TTL Seconds\",\n            info=\"Optional time-to-live for the added texts.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"batch_size\",\n            display_name=\"Batch Size\",\n            info=\"Optional number of data to process in a single batch.\",\n            value=16,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"setup_mode\",\n            display_name=\"Setup Mode\",\n            info=\"Configuration mode for setting up the Cassandra table, with options like 'Sync', 'Async', or 'Off'.\",\n            options=[\"Sync\", \"Async\", \"Off\"],\n            value=\"Sync\",\n            advanced=True,\n        ),\n        DictInput(\n            name=\"cluster_kwargs\",\n            display_name=\"Cluster arguments\",\n            info=\"Optional dictionary of additional keyword arguments for the Cassandra cluster.\",\n            advanced=True,\n            is_list=True,\n        ),\n        MultilineInput(name=\"search_query\", display_name=\"Search Query\"),\n        DataInput(\n            name=\"ingest_data\",\n            display_name=\"Ingest Data\",\n            is_list=True,\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n        IntInput(\n            name=\"number_of_results\",\n            display_name=\"Number of Results\",\n            info=\"Number of results to return.\",\n            value=4,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"search_type\",\n            display_name=\"Search Type\",\n            info=\"Search type to use\",\n            options=[\"Similarity\", \"Similarity with score threshold\", \"MMR (Max Marginal Relevance)\"],\n            value=\"Similarity\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"search_score_threshold\",\n            display_name=\"Search Score Threshold\",\n            info=\"Minimum similarity score threshold for search results. (when using 'Similarity with score threshold')\",\n            value=0,\n            advanced=True,\n        ),\n        DictInput(\n            name=\"search_filter\",\n            display_name=\"Search Metadata Filter\",\n            info=\"Optional dictionary of filters to apply to the search query.\",\n            advanced=True,\n            is_list=True,\n        ),\n        MessageTextInput(\n            name=\"body_search\",\n            display_name=\"Search Body\",\n            info=\"Document textual search terms to apply to the search query.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"enable_body_search\",\n            display_name=\"Enable Body Search\",\n            info=\"Flag to enable body search. This must be enabled BEFORE the table is created.\",\n            value=False,\n            advanced=True,\n        ),\n    ]\n\n    def build_vector_store(self) -> Cassandra:\n        return self._build_cassandra()\n\n    def _build_cassandra(self) -> Cassandra:\n        if self._cached_vectorstore:\n            return self._cached_vectorstore\n        try:\n            import cassio\n            from langchain_community.utilities.cassandra import SetupMode\n        except ImportError:\n            raise ImportError(\n                \"Could not import cassio integration package. \" \"Please install it with `pip install cassio`.\"\n            )\n\n        from uuid import UUID\n\n        database_ref = self.database_ref\n\n        try:\n            UUID(self.database_ref)\n            is_astra = True\n        except ValueError:\n            is_astra = False\n            if \",\" in self.database_ref:\n                # use a copy because we can't change the type of the parameter\n                database_ref = self.database_ref.split(\",\")\n\n        if is_astra:\n            cassio.init(\n                database_id=database_ref,\n                token=self.token,\n                cluster_kwargs=self.cluster_kwargs,\n            )\n        else:\n            cassio.init(\n                contact_points=database_ref,\n                username=self.username,\n                password=self.token,\n                cluster_kwargs=self.cluster_kwargs,\n            )\n        documents = []\n\n        for _input in self.ingest_data or []:\n            if isinstance(_input, Data):\n                documents.append(_input.to_lc_document())\n            else:\n                documents.append(_input)\n\n        if self.enable_body_search:\n            body_index_options = [(\"index_analyzer\", \"STANDARD\")]\n        else:\n            body_index_options = None\n\n        if self.setup_mode == \"Off\":\n            setup_mode = SetupMode.OFF\n        elif self.setup_mode == \"Sync\":\n            setup_mode = SetupMode.SYNC\n        else:\n            setup_mode = SetupMode.ASYNC\n\n        if documents:\n            logger.debug(f\"Adding {len(documents)} documents to the Vector Store.\")\n            table = Cassandra.from_documents(\n                documents=documents,\n                embedding=self.embedding,\n                table_name=self.table_name,\n                keyspace=self.keyspace,\n                ttl_seconds=self.ttl_seconds or None,\n                batch_size=self.batch_size,\n                body_index_options=body_index_options,\n            )\n        else:\n            logger.debug(\"No documents to add to the Vector Store.\")\n            table = Cassandra(\n                embedding=self.embedding,\n                table_name=self.table_name,\n                keyspace=self.keyspace,\n                ttl_seconds=self.ttl_seconds or None,\n                body_index_options=body_index_options,\n                setup_mode=setup_mode,\n            )\n        self._cached_vectorstore = table\n        return table\n\n    def _map_search_type(self):\n        if self.search_type == \"Similarity with score threshold\":\n            return \"similarity_score_threshold\"\n        elif self.search_type == \"MMR (Max Marginal Relevance)\":\n            return \"mmr\"\n        else:\n            return \"similarity\"\n\n    def search_documents(self) -> List[Data]:\n        vector_store = self._build_cassandra()\n\n        logger.debug(f\"Search input: {self.search_query}\")\n        logger.debug(f\"Search type: {self.search_type}\")\n        logger.debug(f\"Number of results: {self.number_of_results}\")\n\n        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():\n            try:\n                search_type = self._map_search_type()\n                search_args = self._build_search_args()\n\n                logger.debug(f\"Search args: {str(search_args)}\")\n\n                docs = vector_store.search(query=self.search_query, search_type=search_type, **search_args)\n            except KeyError as e:\n                if \"content\" in str(e):\n                    raise ValueError(\n                        \"You should ingest data through Langflow (or LangChain) to query it in Langflow. Your collection does not contain a field name 'content'.\"\n                    )\n                else:\n                    raise e\n\n            logger.debug(f\"Retrieved documents: {len(docs)}\")\n\n            data = docs_to_data(docs)\n            self.status = data\n            return data\n        else:\n            return []\n\n    def _build_search_args(self):\n        args = {\n            \"k\": self.number_of_results,\n            \"score_threshold\": self.search_score_threshold,\n        }\n\n        if self.search_filter:\n            clean_filter = {k: v for k, v in self.search_filter.items() if k and v}\n            if len(clean_filter) > 0:\n                args[\"filter\"] = clean_filter\n        if self.body_search:\n            if not self.enable_body_search:\n                raise ValueError(\"You should enable body search when creating the table to search the body field.\")\n            args[\"body_search\"] = self.body_search\n        return args\n\n    def get_retriever_kwargs(self):\n        search_args = self._build_search_args()\n        return {\n            \"search_type\": self._map_search_type(),\n            \"search_kwargs\": search_args,\n        }\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"database_ref":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":true,"placeholder":"","show":true,"value":"cassandra","name":"database_ref","display_name":"Contact Points / Astra Database ID","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Contact points for the database (or AstraDB database ID)","title_case":false,"type":"str","_input_type":"MessageTextInput"},"enable_body_search":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"enable_body_search","display_name":"Enable Body Search","advanced":true,"dynamic":false,"info":"Flag to enable body search. This must be enabled BEFORE the table is created.","title_case":false,"type":"bool","_input_type":"BoolInput"},"keyspace":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":true,"placeholder":"","show":true,"value":"ecopal","name":"keyspace","display_name":"Keyspace","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Table Keyspace (or AstraDB namespace).","title_case":false,"type":"str","_input_type":"MessageTextInput"},"number_of_results":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":4,"name":"number_of_results","display_name":"Number of Results","advanced":true,"dynamic":false,"info":"Number of results to return.","title_case":false,"type":"int","_input_type":"IntInput"},"search_filter":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"value":{},"name":"search_filter","display_name":"Search Metadata Filter","advanced":true,"dynamic":false,"info":"Optional dictionary of filters to apply to the search query.","title_case":false,"type":"dict","_input_type":"DictInput"},"search_query":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"search_query","display_name":"Search Query","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MultilineInput"},"search_score_threshold":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":0,"name":"search_score_threshold","display_name":"Search Score Threshold","advanced":true,"dynamic":false,"info":"Minimum similarity score threshold for search results. (when using 'Similarity with score threshold')","title_case":false,"type":"float","_input_type":"FloatInput"},"search_type":{"trace_as_metadata":true,"options":["Similarity","Similarity with score threshold","MMR (Max Marginal Relevance)"],"combobox":false,"required":false,"placeholder":"","show":true,"value":"Similarity","name":"search_type","display_name":"Search Type","advanced":true,"dynamic":false,"info":"Search type to use","title_case":false,"type":"str","_input_type":"DropdownInput"},"setup_mode":{"trace_as_metadata":true,"options":["Sync","Async","Off"],"combobox":false,"required":false,"placeholder":"","show":true,"value":"Sync","name":"setup_mode","display_name":"Setup Mode","advanced":true,"dynamic":false,"info":"Configuration mode for setting up the Cassandra table, with options like 'Sync', 'Async', or 'Off'.","title_case":false,"type":"str","_input_type":"DropdownInput"},"table_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":true,"placeholder":"","show":true,"value":"vector_store","name":"table_name","display_name":"Table Name","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The name of the table (or AstraDB collection) where vectors will be stored.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"token":{"load_from_db":false,"required":true,"placeholder":"","show":true,"value":"cassandra","name":"token","display_name":"Password / AstraDB Token","advanced":false,"input_types":[],"dynamic":false,"info":"User password for the database (or AstraDB token).","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"ttl_seconds":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"ttl_seconds","display_name":"TTL Seconds","advanced":true,"dynamic":false,"info":"Optional time-to-live for the added texts.","title_case":false,"type":"int","_input_type":"IntInput"},"username":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"cassandra","name":"username","display_name":"Username","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Username for the database (leave empty for AstraDB).","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Cassandra Vector Store with search capabilities","icon":"Cassandra","base_classes":["Data","Retriever","VectorStore"],"display_name":"Cassandra","documentation":"https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/cassandra","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Retriever"],"selected":"Retriever","name":"base_retriever","display_name":"Retriever","method":"build_base_retriever","value":"__UNDEFINED__","cache":true},{"types":["Data"],"selected":"Data","name":"search_results","display_name":"Search Results","method":"search_documents","value":"__UNDEFINED__","cache":true},{"types":["VectorStore"],"selected":"VectorStore","name":"vector_store","display_name":"Vector Store","method":"cast_vector_store","value":"__UNDEFINED__","cache":true}],"field_order":["database_ref","username","token","keyspace","table_name","ttl_seconds","batch_size","setup_mode","cluster_kwargs","search_query","ingest_data","embedding","number_of_results","search_type","search_score_threshold","search_filter","body_search","enable_body_search"],"beta":false,"edited":false},"id":"Cassandra-GJ8X6"},"selected":false,"width":384,"height":1008,"positionAbsolute":{"x":3036.2877189579776,"y":1494.2887273066433},"dragging":false},{"id":"OllamaEmbeddings-gkW72","type":"genericNode","position":{"x":2217.8932611376654,"y":2442.0405854212304},"data":{"type":"OllamaEmbeddings","node":{"template":{"_type":"Component","base_url":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"http://ollama:11434","name":"base_url","display_name":"Ollama Base URL","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain_community.embeddings import OllamaEmbeddings\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import FloatInput, MessageTextInput, Output\n\n\nclass OllamaEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"Ollama Embeddings\"\n    description: str = \"Generate embeddings using Ollama models.\"\n    documentation = \"https://python.langchain.com/docs/integrations/text_embedding/ollama\"\n    icon = \"Ollama\"\n    name = \"OllamaEmbeddings\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"model\",\n            display_name=\"Ollama Model\",\n            value=\"llama2\",\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"Ollama Base URL\",\n            value=\"http://localhost:11434\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Model Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            output = OllamaEmbeddings(\n                model=self.model,\n                base_url=self.base_url,\n                temperature=self.temperature,\n            )  # type: ignore\n        except Exception as e:\n            raise ValueError(\"Could not connect to Ollama API.\") from e\n        return output\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"model":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"llama3:8b","name":"model","display_name":"Ollama Model","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":0.1,"name":"temperature","display_name":"Model Temperature","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generate embeddings using Ollama models.","icon":"Ollama","base_classes":["Embeddings"],"display_name":"Ollama Embeddings","documentation":"https://python.langchain.com/docs/integrations/text_embedding/ollama","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Embeddings"],"selected":"Embeddings","name":"embeddings","display_name":"Embeddings","method":"build_embeddings","value":"__UNDEFINED__","cache":true}],"field_order":["model","base_url","temperature"],"beta":false,"edited":false},"id":"OllamaEmbeddings-gkW72"},"selected":false,"width":384,"height":418,"dragging":false,"positionAbsolute":{"x":2217.8932611376654,"y":2442.0405854212304}},{"id":"Directory-kNUQe","type":"genericNode","position":{"x":1488.9760458757096,"y":1538.6758392385202},"data":{"type":"Directory","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import List\n\nfrom langflow.base.data.utils import parallel_load_data, parse_text_file_to_data, retrieve_file_paths\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, IntInput, MessageTextInput\nfrom langflow.schema import Data\nfrom langflow.template import Output\n\n\nclass DirectoryComponent(Component):\n    display_name = \"Directory\"\n    description = \"Recursively load files from a directory.\"\n    icon = \"folder\"\n    name = \"Directory\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"path\",\n            display_name=\"Path\",\n            info=\"Path to the directory to load files from.\",\n        ),\n        MessageTextInput(\n            name=\"types\",\n            display_name=\"Types\",\n            info=\"File types to load. Leave empty to load all types.\",\n            is_list=True,\n        ),\n        IntInput(\n            name=\"depth\",\n            display_name=\"Depth\",\n            info=\"Depth to search for files.\",\n            value=0,\n        ),\n        IntInput(\n            name=\"max_concurrency\",\n            display_name=\"Max Concurrency\",\n            advanced=True,\n            info=\"Maximum concurrency for loading files.\",\n            value=2,\n        ),\n        BoolInput(\n            name=\"load_hidden\",\n            display_name=\"Load Hidden\",\n            advanced=True,\n            info=\"If true, hidden files will be loaded.\",\n        ),\n        BoolInput(\n            name=\"recursive\",\n            display_name=\"Recursive\",\n            advanced=True,\n            info=\"If true, the search will be recursive.\",\n        ),\n        BoolInput(\n            name=\"silent_errors\",\n            display_name=\"Silent Errors\",\n            advanced=True,\n            info=\"If true, errors will not raise an exception.\",\n        ),\n        BoolInput(\n            name=\"use_multithreading\",\n            display_name=\"Use Multithreading\",\n            advanced=True,\n            info=\"If true, multithreading will be used.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"data\", method=\"load_directory\"),\n    ]\n\n    def load_directory(self) -> List[Data]:\n        path = self.path\n        types = self.types or []  # self.types is already a list due to is_list=True\n        depth = self.depth\n        max_concurrency = self.max_concurrency\n        load_hidden = self.load_hidden\n        recursive = self.recursive\n        silent_errors = self.silent_errors\n        use_multithreading = self.use_multithreading\n\n        resolved_path = self.resolve_path(path)\n        file_paths = retrieve_file_paths(resolved_path, load_hidden, recursive, depth)\n\n        if types:\n            file_paths = [fp for fp in file_paths if any(fp.endswith(ext) for ext in types)]\n\n        loaded_data = []\n\n        if use_multithreading:\n            loaded_data = parallel_load_data(file_paths, silent_errors, max_concurrency)\n        else:\n            loaded_data = [parse_text_file_to_data(file_path, silent_errors) for file_path in file_paths]\n        loaded_data = list(filter(None, loaded_data))\n        self.status = loaded_data\n        return loaded_data  # type: ignore\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"depth":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"10","name":"depth","display_name":"Depth","advanced":false,"dynamic":false,"info":"Depth to search for files.","title_case":false,"type":"int","_input_type":"IntInput"},"load_hidden":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"load_hidden","display_name":"Load Hidden","advanced":true,"dynamic":false,"info":"If true, hidden files will be loaded.","title_case":false,"type":"bool","_input_type":"BoolInput"},"max_concurrency":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":2,"name":"max_concurrency","display_name":"Max Concurrency","advanced":true,"dynamic":false,"info":"Maximum concurrency for loading files.","title_case":false,"type":"int","_input_type":"IntInput"},"path":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"/app/ragdata","name":"path","display_name":"Path","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Path to the directory to load files from.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"recursive":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"recursive","display_name":"Recursive","advanced":true,"dynamic":false,"info":"If true, the search will be recursive.","title_case":false,"type":"bool","_input_type":"BoolInput"},"silent_errors":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"silent_errors","display_name":"Silent Errors","advanced":true,"dynamic":false,"info":"If true, errors will not raise an exception.","title_case":false,"type":"bool","_input_type":"BoolInput"},"types":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":true,"required":false,"placeholder":"","show":true,"value":["md"],"name":"types","display_name":"Types","advanced":false,"input_types":["Message"],"dynamic":false,"info":"File types to load. Leave empty to load all types.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"use_multithreading":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"use_multithreading","display_name":"Use Multithreading","advanced":true,"dynamic":false,"info":"If true, multithreading will be used.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Recursively load files from a directory.","icon":"folder","base_classes":["Data"],"display_name":"Directory","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"data","display_name":"Data","method":"load_directory","value":"__UNDEFINED__","cache":true}],"field_order":["path","types","depth","max_concurrency","load_hidden","recursive","silent_errors","use_multithreading"],"beta":false,"edited":false},"id":"Directory-kNUQe"},"selected":false,"width":384,"height":495},{"id":"Cassandra-ZE9st","type":"genericNode","position":{"x":1221.9073763485962,"y":-150.78216464335077},"data":{"type":"Cassandra","node":{"template":{"_type":"Component","embedding":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"embedding","display_name":"Embedding","advanced":false,"input_types":["Embeddings"],"dynamic":false,"info":"","title_case":false,"type":"other","_input_type":"HandleInput"},"ingest_data":{"trace_as_input":true,"trace_as_metadata":true,"list":true,"required":false,"placeholder":"","show":true,"value":"","name":"ingest_data","display_name":"Ingest Data","advanced":false,"input_types":["Data"],"dynamic":false,"info":"","title_case":false,"type":"other","_input_type":"DataInput"},"batch_size":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":16,"name":"batch_size","display_name":"Batch Size","advanced":true,"dynamic":false,"info":"Optional number of data to process in a single batch.","title_case":false,"type":"int","_input_type":"IntInput"},"body_search":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"body_search","display_name":"Search Body","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Document textual search terms to apply to the search query.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"cluster_kwargs":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"value":{},"name":"cluster_kwargs","display_name":"Cluster arguments","advanced":true,"dynamic":false,"info":"Optional dictionary of additional keyword arguments for the Cassandra cluster.","title_case":false,"type":"dict","_input_type":"DictInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import List\n\nfrom langchain_community.vectorstores import Cassandra\nfrom loguru import logger\n\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.inputs import BoolInput, DictInput, FloatInput\nfrom langflow.io import (\n    DataInput,\n    DropdownInput,\n    HandleInput,\n    IntInput,\n    MessageTextInput,\n    MultilineInput,\n    SecretStrInput,\n)\nfrom langflow.schema import Data\n\n\nclass CassandraVectorStoreComponent(LCVectorStoreComponent):\n    display_name = \"Cassandra\"\n    description = \"Cassandra Vector Store with search capabilities\"\n    documentation = \"https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/cassandra\"\n    name = \"Cassandra\"\n    icon = \"Cassandra\"\n\n    _cached_vectorstore: Cassandra | None = None\n\n    inputs = [\n        MessageTextInput(\n            name=\"database_ref\",\n            display_name=\"Contact Points / Astra Database ID\",\n            info=\"Contact points for the database (or AstraDB database ID)\",\n            required=True,\n        ),\n        MessageTextInput(\n            name=\"username\", display_name=\"Username\", info=\"Username for the database (leave empty for AstraDB).\"\n        ),\n        SecretStrInput(\n            name=\"token\",\n            display_name=\"Password / AstraDB Token\",\n            info=\"User password for the database (or AstraDB token).\",\n            required=True,\n        ),\n        MessageTextInput(\n            name=\"keyspace\",\n            display_name=\"Keyspace\",\n            info=\"Table Keyspace (or AstraDB namespace).\",\n            required=True,\n        ),\n        MessageTextInput(\n            name=\"table_name\",\n            display_name=\"Table Name\",\n            info=\"The name of the table (or AstraDB collection) where vectors will be stored.\",\n            required=True,\n        ),\n        IntInput(\n            name=\"ttl_seconds\",\n            display_name=\"TTL Seconds\",\n            info=\"Optional time-to-live for the added texts.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"batch_size\",\n            display_name=\"Batch Size\",\n            info=\"Optional number of data to process in a single batch.\",\n            value=16,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"setup_mode\",\n            display_name=\"Setup Mode\",\n            info=\"Configuration mode for setting up the Cassandra table, with options like 'Sync', 'Async', or 'Off'.\",\n            options=[\"Sync\", \"Async\", \"Off\"],\n            value=\"Sync\",\n            advanced=True,\n        ),\n        DictInput(\n            name=\"cluster_kwargs\",\n            display_name=\"Cluster arguments\",\n            info=\"Optional dictionary of additional keyword arguments for the Cassandra cluster.\",\n            advanced=True,\n            is_list=True,\n        ),\n        MultilineInput(name=\"search_query\", display_name=\"Search Query\"),\n        DataInput(\n            name=\"ingest_data\",\n            display_name=\"Ingest Data\",\n            is_list=True,\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n        IntInput(\n            name=\"number_of_results\",\n            display_name=\"Number of Results\",\n            info=\"Number of results to return.\",\n            value=4,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"search_type\",\n            display_name=\"Search Type\",\n            info=\"Search type to use\",\n            options=[\"Similarity\", \"Similarity with score threshold\", \"MMR (Max Marginal Relevance)\"],\n            value=\"Similarity\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"search_score_threshold\",\n            display_name=\"Search Score Threshold\",\n            info=\"Minimum similarity score threshold for search results. (when using 'Similarity with score threshold')\",\n            value=0,\n            advanced=True,\n        ),\n        DictInput(\n            name=\"search_filter\",\n            display_name=\"Search Metadata Filter\",\n            info=\"Optional dictionary of filters to apply to the search query.\",\n            advanced=True,\n            is_list=True,\n        ),\n        MessageTextInput(\n            name=\"body_search\",\n            display_name=\"Search Body\",\n            info=\"Document textual search terms to apply to the search query.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"enable_body_search\",\n            display_name=\"Enable Body Search\",\n            info=\"Flag to enable body search. This must be enabled BEFORE the table is created.\",\n            value=False,\n            advanced=True,\n        ),\n    ]\n\n    def build_vector_store(self) -> Cassandra:\n        return self._build_cassandra()\n\n    def _build_cassandra(self) -> Cassandra:\n        if self._cached_vectorstore:\n            return self._cached_vectorstore\n        try:\n            import cassio\n            from langchain_community.utilities.cassandra import SetupMode\n        except ImportError:\n            raise ImportError(\n                \"Could not import cassio integration package. \" \"Please install it with `pip install cassio`.\"\n            )\n\n        from uuid import UUID\n\n        database_ref = self.database_ref\n\n        try:\n            UUID(self.database_ref)\n            is_astra = True\n        except ValueError:\n            is_astra = False\n            if \",\" in self.database_ref:\n                # use a copy because we can't change the type of the parameter\n                database_ref = self.database_ref.split(\",\")\n\n        if is_astra:\n            cassio.init(\n                database_id=database_ref,\n                token=self.token,\n                cluster_kwargs=self.cluster_kwargs,\n            )\n        else:\n            cassio.init(\n                contact_points=database_ref,\n                username=self.username,\n                password=self.token,\n                cluster_kwargs=self.cluster_kwargs,\n            )\n        documents = []\n\n        for _input in self.ingest_data or []:\n            if isinstance(_input, Data):\n                documents.append(_input.to_lc_document())\n            else:\n                documents.append(_input)\n\n        if self.enable_body_search:\n            body_index_options = [(\"index_analyzer\", \"STANDARD\")]\n        else:\n            body_index_options = None\n\n        if self.setup_mode == \"Off\":\n            setup_mode = SetupMode.OFF\n        elif self.setup_mode == \"Sync\":\n            setup_mode = SetupMode.SYNC\n        else:\n            setup_mode = SetupMode.ASYNC\n\n        if documents:\n            logger.debug(f\"Adding {len(documents)} documents to the Vector Store.\")\n            table = Cassandra.from_documents(\n                documents=documents,\n                embedding=self.embedding,\n                table_name=self.table_name,\n                keyspace=self.keyspace,\n                ttl_seconds=self.ttl_seconds or None,\n                batch_size=self.batch_size,\n                body_index_options=body_index_options,\n            )\n        else:\n            logger.debug(\"No documents to add to the Vector Store.\")\n            table = Cassandra(\n                embedding=self.embedding,\n                table_name=self.table_name,\n                keyspace=self.keyspace,\n                ttl_seconds=self.ttl_seconds or None,\n                body_index_options=body_index_options,\n                setup_mode=setup_mode,\n            )\n        self._cached_vectorstore = table\n        return table\n\n    def _map_search_type(self):\n        if self.search_type == \"Similarity with score threshold\":\n            return \"similarity_score_threshold\"\n        elif self.search_type == \"MMR (Max Marginal Relevance)\":\n            return \"mmr\"\n        else:\n            return \"similarity\"\n\n    def search_documents(self) -> List[Data]:\n        vector_store = self._build_cassandra()\n\n        logger.debug(f\"Search input: {self.search_query}\")\n        logger.debug(f\"Search type: {self.search_type}\")\n        logger.debug(f\"Number of results: {self.number_of_results}\")\n\n        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():\n            try:\n                search_type = self._map_search_type()\n                search_args = self._build_search_args()\n\n                logger.debug(f\"Search args: {str(search_args)}\")\n\n                docs = vector_store.search(query=self.search_query, search_type=search_type, **search_args)\n            except KeyError as e:\n                if \"content\" in str(e):\n                    raise ValueError(\n                        \"You should ingest data through Langflow (or LangChain) to query it in Langflow. Your collection does not contain a field name 'content'.\"\n                    )\n                else:\n                    raise e\n\n            logger.debug(f\"Retrieved documents: {len(docs)}\")\n\n            data = docs_to_data(docs)\n            self.status = data\n            return data\n        else:\n            return []\n\n    def _build_search_args(self):\n        args = {\n            \"k\": self.number_of_results,\n            \"score_threshold\": self.search_score_threshold,\n        }\n\n        if self.search_filter:\n            clean_filter = {k: v for k, v in self.search_filter.items() if k and v}\n            if len(clean_filter) > 0:\n                args[\"filter\"] = clean_filter\n        if self.body_search:\n            if not self.enable_body_search:\n                raise ValueError(\"You should enable body search when creating the table to search the body field.\")\n            args[\"body_search\"] = self.body_search\n        return args\n\n    def get_retriever_kwargs(self):\n        search_args = self._build_search_args()\n        return {\n            \"search_type\": self._map_search_type(),\n            \"search_kwargs\": search_args,\n        }\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"database_ref":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":true,"placeholder":"","show":true,"value":"cassandra","name":"database_ref","display_name":"Contact Points / Astra Database ID","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Contact points for the database (or AstraDB database ID)","title_case":false,"type":"str","_input_type":"MessageTextInput"},"enable_body_search":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"enable_body_search","display_name":"Enable Body Search","advanced":true,"dynamic":false,"info":"Flag to enable body search. This must be enabled BEFORE the table is created.","title_case":false,"type":"bool","_input_type":"BoolInput"},"keyspace":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":true,"placeholder":"","show":true,"value":"ecopal","name":"keyspace","display_name":"Keyspace","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Table Keyspace (or AstraDB namespace).","title_case":false,"type":"str","_input_type":"MessageTextInput"},"number_of_results":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":4,"name":"number_of_results","display_name":"Number of Results","advanced":true,"dynamic":false,"info":"Number of results to return.","title_case":false,"type":"int","_input_type":"IntInput"},"search_filter":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"value":{},"name":"search_filter","display_name":"Search Metadata Filter","advanced":true,"dynamic":false,"info":"Optional dictionary of filters to apply to the search query.","title_case":false,"type":"dict","_input_type":"DictInput"},"search_query":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"search_query","display_name":"Search Query","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MultilineInput"},"search_score_threshold":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":0,"name":"search_score_threshold","display_name":"Search Score Threshold","advanced":true,"dynamic":false,"info":"Minimum similarity score threshold for search results. (when using 'Similarity with score threshold')","title_case":false,"type":"float","_input_type":"FloatInput"},"search_type":{"trace_as_metadata":true,"options":["Similarity","Similarity with score threshold","MMR (Max Marginal Relevance)"],"combobox":false,"required":false,"placeholder":"","show":true,"value":"Similarity","name":"search_type","display_name":"Search Type","advanced":true,"dynamic":false,"info":"Search type to use","title_case":false,"type":"str","_input_type":"DropdownInput"},"setup_mode":{"trace_as_metadata":true,"options":["Sync","Async","Off"],"combobox":false,"required":false,"placeholder":"","show":true,"value":"Sync","name":"setup_mode","display_name":"Setup Mode","advanced":true,"dynamic":false,"info":"Configuration mode for setting up the Cassandra table, with options like 'Sync', 'Async', or 'Off'.","title_case":false,"type":"str","_input_type":"DropdownInput"},"table_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":true,"placeholder":"","show":true,"value":"vector_store","name":"table_name","display_name":"Table Name","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The name of the table (or AstraDB collection) where vectors will be stored.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"token":{"load_from_db":false,"required":true,"placeholder":"","show":true,"value":"cassandra","name":"token","display_name":"Password / AstraDB Token","advanced":false,"input_types":[],"dynamic":false,"info":"User password for the database (or AstraDB token).","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"ttl_seconds":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"ttl_seconds","display_name":"TTL Seconds","advanced":true,"dynamic":false,"info":"Optional time-to-live for the added texts.","title_case":false,"type":"int","_input_type":"IntInput"},"username":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"cassandra","name":"username","display_name":"Username","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Username for the database (leave empty for AstraDB).","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Cassandra Vector Store with search capabilities","icon":"Cassandra","base_classes":["Data","Retriever","VectorStore"],"display_name":"Cassandra","documentation":"https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/cassandra","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Retriever"],"selected":"Retriever","name":"base_retriever","display_name":"Retriever","method":"build_base_retriever","value":"__UNDEFINED__","cache":true},{"types":["Data"],"selected":"Data","name":"search_results","display_name":"Search Results","method":"search_documents","value":"__UNDEFINED__","cache":true},{"types":["VectorStore"],"selected":"VectorStore","name":"vector_store","display_name":"Vector Store","method":"cast_vector_store","value":"__UNDEFINED__","cache":true}],"field_order":["database_ref","username","token","keyspace","table_name","ttl_seconds","batch_size","setup_mode","cluster_kwargs","search_query","ingest_data","embedding","number_of_results","search_type","search_score_threshold","search_filter","body_search","enable_body_search"],"beta":false,"edited":false},"id":"Cassandra-ZE9st"},"selected":false,"width":384,"height":1008,"positionAbsolute":{"x":1221.9073763485962,"y":-150.78216464335077},"dragging":false},{"id":"OllamaEmbeddings-M4UFL","type":"genericNode","position":{"x":380.69827132994806,"y":1373.2886119860382},"data":{"type":"OllamaEmbeddings","node":{"template":{"_type":"Component","base_url":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"http://ollama:11434","name":"base_url","display_name":"Ollama Base URL","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain_community.embeddings import OllamaEmbeddings\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import FloatInput, MessageTextInput, Output\n\n\nclass OllamaEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"Ollama Embeddings\"\n    description: str = \"Generate embeddings using Ollama models.\"\n    documentation = \"https://python.langchain.com/docs/integrations/text_embedding/ollama\"\n    icon = \"Ollama\"\n    name = \"OllamaEmbeddings\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"model\",\n            display_name=\"Ollama Model\",\n            value=\"llama2\",\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"Ollama Base URL\",\n            value=\"http://localhost:11434\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Model Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            output = OllamaEmbeddings(\n                model=self.model,\n                base_url=self.base_url,\n                temperature=self.temperature,\n            )  # type: ignore\n        except Exception as e:\n            raise ValueError(\"Could not connect to Ollama API.\") from e\n        return output\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"model":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"llama3:8b","name":"model","display_name":"Ollama Model","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":0.1,"name":"temperature","display_name":"Model Temperature","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generate embeddings using Ollama models.","icon":"Ollama","base_classes":["Embeddings"],"display_name":"Ollama Embeddings","documentation":"https://python.langchain.com/docs/integrations/text_embedding/ollama","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Embeddings"],"selected":"Embeddings","name":"embeddings","display_name":"Embeddings","method":"build_embeddings","value":"__UNDEFINED__","cache":true}],"field_order":["model","base_url","temperature"],"beta":false,"edited":false},"id":"OllamaEmbeddings-M4UFL"},"selected":false,"width":384,"height":418,"dragging":false,"positionAbsolute":{"x":380.69827132994806,"y":1373.2886119860382}}],"edges":[{"className":"","data":{"sourceHandle":{"dataType":"ParseData","id":"ParseData-QUatH","name":"text","output_types":["Message"]},"targetHandle":{"fieldName":"context","id":"Prompt-nwpd4","inputTypes":["Message","Text"],"type":"str"}},"id":"reactflow__edge-ParseData-QUatH{dataType:ParseData,id:ParseData-QUatH,name:text,output_types:[Message]}-Prompt-nwpd4{fieldName:context,id:Prompt-nwpd4,inputTypes:[Message,Text],type:str}","source":"ParseData-QUatH","sourceHandle":"{dataType:ParseData,id:ParseData-QUatH,name:text,output_types:[Message]}","target":"Prompt-nwpd4","targetHandle":"{fieldName:context,id:Prompt-nwpd4,inputTypes:[Message,Text],type:str}"},{"className":"","data":{"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-pcMgt","name":"message","output_types":["Message"]},"targetHandle":{"fieldName":"question","id":"Prompt-nwpd4","inputTypes":["Message","Text"],"type":"str"}},"id":"reactflow__edge-ChatInput-pcMgt{dataType:ChatInput,id:ChatInput-pcMgt,name:message,output_types:[Message]}-Prompt-nwpd4{fieldName:question,id:Prompt-nwpd4,inputTypes:[Message,Text],type:str}","source":"ChatInput-pcMgt","sourceHandle":"{dataType:ChatInput,id:ChatInput-pcMgt,name:message,output_types:[Message]}","target":"Prompt-nwpd4","targetHandle":"{fieldName:question,id:Prompt-nwpd4,inputTypes:[Message,Text],type:str}"},{"source":"Prompt-nwpd4","sourceHandle":"{dataType:Prompt,id:Prompt-nwpd4,name:prompt,output_types:[Message]}","target":"OllamaModel-OKERI","targetHandle":"{fieldName:input_value,id:OllamaModel-OKERI,inputTypes:[Message],type:str}","data":{"targetHandle":{"fieldName":"input_value","id":"OllamaModel-OKERI","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-nwpd4","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-nwpd4{dataType:Prompt,id:Prompt-nwpd4,name:prompt,output_types:[Message]}-OllamaModel-OKERI{fieldName:input_value,id:OllamaModel-OKERI,inputTypes:[Message],type:str}","className":""},{"source":"OllamaModel-OKERI","sourceHandle":"{dataType:OllamaModel,id:OllamaModel-OKERI,name:text_output,output_types:[Message]}","target":"ChatOutput-c1Jrr","targetHandle":"{fieldName:input_value,id:ChatOutput-c1Jrr,inputTypes:[Message],type:str}","data":{"targetHandle":{"fieldName":"input_value","id":"ChatOutput-c1Jrr","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"OllamaModel","id":"OllamaModel-OKERI","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-OllamaModel-OKERI{dataType:OllamaModel,id:OllamaModel-OKERI,name:text_output,output_types:[Message]}-ChatOutput-c1Jrr{fieldName:input_value,id:ChatOutput-c1Jrr,inputTypes:[Message],type:str}","className":""},{"source":"SplitText-uDC7v","sourceHandle":"{dataType:SplitText,id:SplitText-uDC7v,name:chunks,output_types:[Data]}","target":"Cassandra-GJ8X6","targetHandle":"{fieldName:ingest_data,id:Cassandra-GJ8X6,inputTypes:[Data],type:other}","data":{"targetHandle":{"fieldName":"ingest_data","id":"Cassandra-GJ8X6","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"SplitText","id":"SplitText-uDC7v","name":"chunks","output_types":["Data"]}},"id":"reactflow__edge-SplitText-uDC7v{dataType:SplitText,id:SplitText-uDC7v,name:chunks,output_types:[Data]}-Cassandra-GJ8X6{fieldName:ingest_data,id:Cassandra-GJ8X6,inputTypes:[Data],type:other}","className":""},{"source":"OllamaEmbeddings-gkW72","sourceHandle":"{dataType:OllamaEmbeddings,id:OllamaEmbeddings-gkW72,name:embeddings,output_types:[Embeddings]}","target":"Cassandra-GJ8X6","targetHandle":"{fieldName:embedding,id:Cassandra-GJ8X6,inputTypes:[Embeddings],type:other}","data":{"targetHandle":{"fieldName":"embedding","id":"Cassandra-GJ8X6","inputTypes":["Embeddings"],"type":"other"},"sourceHandle":{"dataType":"OllamaEmbeddings","id":"OllamaEmbeddings-gkW72","name":"embeddings","output_types":["Embeddings"]}},"id":"reactflow__edge-OllamaEmbeddings-gkW72{dataType:OllamaEmbeddings,id:OllamaEmbeddings-gkW72,name:embeddings,output_types:[Embeddings]}-Cassandra-GJ8X6{fieldName:embedding,id:Cassandra-GJ8X6,inputTypes:[Embeddings],type:other}","className":""},{"source":"Directory-kNUQe","sourceHandle":"{dataType:Directory,id:Directory-kNUQe,name:data,output_types:[Data]}","target":"SplitText-uDC7v","targetHandle":"{fieldName:data_inputs,id:SplitText-uDC7v,inputTypes:[Data],type:other}","data":{"targetHandle":{"fieldName":"data_inputs","id":"SplitText-uDC7v","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"Directory","id":"Directory-kNUQe","name":"data","output_types":["Data"]}},"id":"reactflow__edge-Directory-kNUQe{dataType:Directory,id:Directory-kNUQe,name:data,output_types:[Data]}-SplitText-uDC7v{fieldName:data_inputs,id:SplitText-uDC7v,inputTypes:[Data],type:other}","className":""},{"source":"OllamaEmbeddings-M4UFL","sourceHandle":"{dataType:OllamaEmbeddings,id:OllamaEmbeddings-M4UFL,name:embeddings,output_types:[Embeddings]}","target":"Cassandra-ZE9st","targetHandle":"{fieldName:embedding,id:Cassandra-ZE9st,inputTypes:[Embeddings],type:other}","data":{"targetHandle":{"fieldName":"embedding","id":"Cassandra-ZE9st","inputTypes":["Embeddings"],"type":"other"},"sourceHandle":{"dataType":"OllamaEmbeddings","id":"OllamaEmbeddings-M4UFL","name":"embeddings","output_types":["Embeddings"]}},"id":"reactflow__edge-OllamaEmbeddings-M4UFL{dataType:OllamaEmbeddings,id:OllamaEmbeddings-M4UFL,name:embeddings,output_types:[Embeddings]}-Cassandra-ZE9st{fieldName:embedding,id:Cassandra-ZE9st,inputTypes:[Embeddings],type:other}","className":""},{"source":"ChatInput-pcMgt","sourceHandle":"{dataType:ChatInput,id:ChatInput-pcMgt,name:message,output_types:[Message]}","target":"Cassandra-ZE9st","targetHandle":"{fieldName:search_query,id:Cassandra-ZE9st,inputTypes:[Message],type:str}","data":{"targetHandle":{"fieldName":"search_query","id":"Cassandra-ZE9st","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-pcMgt","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-pcMgt{dataType:ChatInput,id:ChatInput-pcMgt,name:message,output_types:[Message]}-Cassandra-ZE9st{fieldName:search_query,id:Cassandra-ZE9st,inputTypes:[Message],type:str}","className":""},{"source":"Cassandra-ZE9st","sourceHandle":"{dataType:Cassandra,id:Cassandra-ZE9st,name:search_results,output_types:[Data]}","target":"ParseData-QUatH","targetHandle":"{fieldName:data,id:ParseData-QUatH,inputTypes:[Data],type:other}","data":{"targetHandle":{"fieldName":"data","id":"ParseData-QUatH","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"Cassandra","id":"Cassandra-ZE9st","name":"search_results","output_types":["Data"]}},"id":"reactflow__edge-Cassandra-ZE9st{dataType:Cassandra,id:Cassandra-ZE9st,name:search_results,output_types:[Data]}-ParseData-QUatH{fieldName:data,id:ParseData-QUatH,inputTypes:[Data],type:other}","className":""}],"viewport":{"x":-919.9074929026657,"y":-624.488906276651,"zoom":0.7122284349002787}},"description":"Visit https://docs.langflow.org/tutorials/rag-with-astradb for a detailed guide of this project.\nThis project give you both Ingestion and RAG in a single file. You'll need to visit https://astra.datastax.com/ to create an Astra DB instance, your Token and grab an API Endpoint.\nRunning this project requires you to add a file in the Files component, then define a Collection Name and click on the Play icon on the Astra DB component. \n\nAfter the ingestion ends you are ready to click on the Run button at the lower left corner and start asking questions about your data.","name":"Vector Store RAG","last_tested_version":"1.0.14","endpoint_name":null,"is_component":false}